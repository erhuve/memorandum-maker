{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#NOTICE: This code follows the tutorial from TensorFlow on generating text\n",
    "# They're very cool. Here's their license:\n",
    "#\n",
    "#@title Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "# https://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dependencies\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = open('data.txt', encoding='utf-8').read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Academics\nResearch\nThe gradual ramp-up of in-person research is continuing.\nGlobal: For fall 2020, \"Go Local\" is a \"go\", Study Away is a \"no\" (mostly)\nOwing to ongoing international travel restrictions and related health complexities, the University \n"
    }
   ],
   "source": [
    "# print first 250 characters juuuust to make sure\n",
    "print(corpus[:250])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "84 unique characters\n"
    }
   ],
   "source": [
    "# unique characters in file\n",
    "vocab = sorted(set(corpus))\n",
    "print ('{} unique characters'.format(len(vocab)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# map characters to indices and vice versa for feeding into LSTM and interpreting afterward\n",
    "char2idx = {u:i for i, u in enumerate(vocab)}\n",
    "idx2char = np.array(vocab)\n",
    "\n",
    "text_as_int = np.array([char2idx[c] for c in corpus])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "'Academics\\nRes' ---- characters mapped to int ---- > [28 55 53 56 57 65 61 55 71  0 44 57 71]\n"
    }
   ],
   "source": [
    "# show how the first 13 characters from the text are mapped to integers juuuust to make sure\n",
    "print ('{} ---- characters mapped to int ---- > {}'.format(repr(corpus[:13]), text_as_int[:13]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "A\nc\na\nd\ne\n"
    }
   ],
   "source": [
    "# maximum length sentence we want for a single input in characters\n",
    "seq_length = 100\n",
    "examples_per_epoch = len(corpus)//(seq_length+1)\n",
    "\n",
    "# create training examples / targets\n",
    "char_dataset = tf.data.Dataset.from_tensor_slices(text_as_int)\n",
    "\n",
    "for i in char_dataset.take(5):\n",
    "  print(idx2char[i.numpy()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "'Academics\\nResearch\\nThe gradual ramp-up of in-person research is continuing.\\nGlobal: For fall 2020, \"G'\n'o Local\" is a \"go\", Study Away is a \"no\" (mostly)\\nOwing to ongoing international travel restrictions '\n'and related health complexities, the University has notified students enrolled in Study Away for fall'\n' 2020 that it is limiting participation to students who do not require issuance of a student visa or '\n'who are already present in the host country or region where they were intending to study. \\n\\nIn concer'\n"
    }
   ],
   "source": [
    "sequences = char_dataset.batch(seq_length+1, drop_remainder=True)\n",
    "\n",
    "for item in sequences.take(5):\n",
    "  print(repr(''.join(idx2char[item.numpy()])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_input_target(chunk):\n",
    "    input_text = chunk[:-1]\n",
    "    target_text = chunk[1:]\n",
    "    return input_text, target_text\n",
    "\n",
    "dataset = sequences.map(split_input_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "<BatchDataset shapes: ((32, 100), (32, 100)), types: (tf.int64, tf.int64)>"
     },
     "metadata": {},
     "execution_count": 12
    }
   ],
   "source": [
    "# batch size\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "# buffer size to shuffle the dataset\n",
    "# (TF data is designed to work with possibly infinite sequences,\n",
    "# so it doesn't attempt to shuffle the entire sequence in memory. Instead,\n",
    "# it maintains a buffer in which it shuffles elements). Thanks TF!\n",
    "BUFFER_SIZE = 10000\n",
    "\n",
    "dataset = dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder=True)\n",
    "\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# length of the vocabulary in chars\n",
    "vocab_size = len(vocab)\n",
    "\n",
    "# embedding dimension\n",
    "embedding_dim = 256\n",
    "\n",
    "# number of RNN units\n",
    "rnn_units = 1024\n",
    "\n",
    "# number of epochs\n",
    "EPOCHS = 75"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(vocab_size, embedding_dim, rnn_units, batch_size):\n",
    "  model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Embedding(vocab_size, embedding_dim,\n",
    "                              batch_input_shape=[batch_size, None]),\n",
    "    tf.keras.layers.GRU(rnn_units,\n",
    "                        return_sequences=True,\n",
    "                        stateful=True,\n",
    "                        recurrent_initializer='glorot_uniform'),\n",
    "    tf.keras.layers.Dense(vocab_size)\n",
    "  ])\n",
    "  return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Model: \"sequential\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\nembedding (Embedding)        (32, None, 256)           21504     \n_________________________________________________________________\ngru (GRU)                    (32, None, 1024)          3938304   \n_________________________________________________________________\ndense (Dense)                (32, None, 84)            86100     \n=================================================================\nTotal params: 4,045,908\nTrainable params: 4,045,908\nNon-trainable params: 0\n_________________________________________________________________\n"
    }
   ],
   "source": [
    "model = build_model(\n",
    "  vocab_size = len(vocab),\n",
    "  embedding_dim=embedding_dim,\n",
    "  rnn_units=rnn_units,\n",
    "  batch_size=BATCH_SIZE)\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss(labels, logits):\n",
    "  return tf.keras.losses.sparse_categorical_crossentropy(labels, logits, from_logits=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam', loss=loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# directory where the checkpoints will be saved\n",
    "checkpoint_dir = './training_checkpoints'\n",
    "# name of the checkpoint files\n",
    "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt_{epoch}\")\n",
    "\n",
    "checkpoint_callback=tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath=checkpoint_prefix,\n",
    "    save_weights_only=True)\n",
    "\n",
    "# regular callback\n",
    "callback = tf.keras.callbacks.EarlyStopping(monitor='loss', patience=7)\n",
    "\n",
    "callbacks_list = [checkpoint_callback, callback]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Epoch 1/75\n12/12 [==============================] - 28s 2s/step - loss: 4.3515\nEpoch 2/75\n12/12 [==============================] - 36s 3s/step - loss: 3.5774\nEpoch 3/75\n12/12 [==============================] - 40s 3s/step - loss: 3.0264\nEpoch 4/75\n12/12 [==============================] - 86s 7s/step - loss: 2.8357\nEpoch 5/75\n12/12 [==============================] - 39s 3s/step - loss: 2.6559\nEpoch 6/75\n12/12 [==============================] - 33s 3s/step - loss: 2.5309\nEpoch 7/75\n12/12 [==============================] - 44s 4s/step - loss: 2.4456\nEpoch 8/75\n12/12 [==============================] - 34s 3s/step - loss: 2.3779\nEpoch 9/75\n12/12 [==============================] - 37s 3s/step - loss: 2.3172\nEpoch 10/75\n12/12 [==============================] - 30s 2s/step - loss: 2.2607\nEpoch 11/75\n12/12 [==============================] - 32s 3s/step - loss: 2.2046\nEpoch 12/75\n12/12 [==============================] - 31s 3s/step - loss: 2.1470\nEpoch 13/75\n12/12 [==============================] - 40s 3s/step - loss: 2.0881\nEpoch 14/75\n12/12 [==============================] - 32s 3s/step - loss: 2.0241\nEpoch 15/75\n12/12 [==============================] - 34s 3s/step - loss: 1.9642\nEpoch 16/75\n12/12 [==============================] - 28s 2s/step - loss: 1.8997\nEpoch 17/75\n12/12 [==============================] - 32s 3s/step - loss: 1.8360\nEpoch 18/75\n12/12 [==============================] - 25s 2s/step - loss: 1.7696\nEpoch 19/75\n12/12 [==============================] - 28s 2s/step - loss: 1.7054\nEpoch 20/75\n12/12 [==============================] - 27s 2s/step - loss: 1.6400\nEpoch 21/75\n12/12 [==============================] - 25s 2s/step - loss: 1.5753\nEpoch 22/75\n12/12 [==============================] - 28s 2s/step - loss: 1.5107\nEpoch 23/75\n12/12 [==============================] - 31s 3s/step - loss: 1.4458\nEpoch 24/75\n12/12 [==============================] - 37s 3s/step - loss: 1.3822\nEpoch 25/75\n12/12 [==============================] - 30s 2s/step - loss: 1.3159\nEpoch 26/75\n12/12 [==============================] - 26s 2s/step - loss: 1.2538\nEpoch 27/75\n12/12 [==============================] - 24s 2s/step - loss: 1.1928\nEpoch 28/75\n12/12 [==============================] - 31s 3s/step - loss: 1.1308\nEpoch 29/75\n12/12 [==============================] - 29s 2s/step - loss: 1.0691\nEpoch 30/75\n12/12 [==============================] - 36s 3s/step - loss: 1.0077\nEpoch 31/75\n12/12 [==============================] - 33s 3s/step - loss: 0.9538\nEpoch 32/75\n12/12 [==============================] - 29s 2s/step - loss: 0.8906\nEpoch 33/75\n12/12 [==============================] - 25s 2s/step - loss: 0.8346\nEpoch 34/75\n12/12 [==============================] - 24s 2s/step - loss: 0.7801\nEpoch 35/75\n12/12 [==============================] - 28s 2s/step - loss: 0.7225\nEpoch 36/75\n12/12 [==============================] - 29s 2s/step - loss: 0.6656\nEpoch 37/75\n12/12 [==============================] - 30s 3s/step - loss: 0.6117\nEpoch 38/75\n12/12 [==============================] - 30s 2s/step - loss: 0.5644\nEpoch 39/75\n12/12 [==============================] - 28s 2s/step - loss: 0.5189\nEpoch 40/75\n12/12 [==============================] - 20s 2s/step - loss: 0.4769\nEpoch 41/75\n12/12 [==============================] - 30s 3s/step - loss: 0.4389\nEpoch 42/75\n12/12 [==============================] - 30s 3s/step - loss: 0.4006\nEpoch 43/75\n12/12 [==============================] - 30s 3s/step - loss: 0.3684\nEpoch 44/75\n12/12 [==============================] - 33s 3s/step - loss: 0.3488\nEpoch 45/75\n12/12 [==============================] - 25s 2s/step - loss: 0.3208\nEpoch 46/75\n12/12 [==============================] - 24s 2s/step - loss: 0.3028\nEpoch 47/75\n12/12 [==============================] - 25s 2s/step - loss: 0.2898\nEpoch 48/75\n12/12 [==============================] - 28s 2s/step - loss: 0.2680\nEpoch 49/75\n12/12 [==============================] - 25s 2s/step - loss: 0.2527\nEpoch 50/75\n12/12 [==============================] - 25s 2s/step - loss: 0.2415\nEpoch 51/75\n12/12 [==============================] - 24s 2s/step - loss: 0.2316\nEpoch 52/75\n12/12 [==============================] - 29s 2s/step - loss: 0.2253\nEpoch 53/75\n12/12 [==============================] - 29s 2s/step - loss: 0.2121\nEpoch 54/75\n12/12 [==============================] - 25s 2s/step - loss: 0.2052\nEpoch 55/75\n12/12 [==============================] - 24s 2s/step - loss: 0.1987\nEpoch 56/75\n12/12 [==============================] - 24s 2s/step - loss: 0.1906\nEpoch 57/75\n12/12 [==============================] - 24s 2s/step - loss: 0.1850\nEpoch 58/75\n12/12 [==============================] - 26s 2s/step - loss: 0.1828\nEpoch 59/75\n12/12 [==============================] - 23s 2s/step - loss: 0.1771\nEpoch 60/75\n12/12 [==============================] - 28s 2s/step - loss: 0.1749\nEpoch 61/75\n12/12 [==============================] - 24s 2s/step - loss: 0.1686\nEpoch 62/75\n12/12 [==============================] - 22s 2s/step - loss: 0.1665\nEpoch 63/75\n12/12 [==============================] - 25s 2s/step - loss: 0.1631\nEpoch 64/75\n12/12 [==============================] - 28s 2s/step - loss: 0.1556\nEpoch 65/75\n12/12 [==============================] - 45s 4s/step - loss: 0.1550\nEpoch 66/75\n12/12 [==============================] - 21s 2s/step - loss: 0.1541\nEpoch 67/75\n12/12 [==============================] - 27s 2s/step - loss: 0.1469\nEpoch 68/75\n12/12 [==============================] - 30s 2s/step - loss: 0.1479\nEpoch 69/75\n12/12 [==============================] - 34s 3s/step - loss: 0.1443\nEpoch 70/75\n12/12 [==============================] - 31s 3s/step - loss: 0.1458\nEpoch 71/75\n12/12 [==============================] - 24s 2s/step - loss: 0.1434\nEpoch 72/75\n12/12 [==============================] - 29s 2s/step - loss: 0.1412\nEpoch 73/75\n12/12 [==============================] - 33s 3s/step - loss: 0.1381\nEpoch 74/75\n12/12 [==============================] - 23s 2s/step - loss: 0.1325\nEpoch 75/75\n12/12 [==============================] - 26s 2s/step - loss: 0.1360\n"
    }
   ],
   "source": [
    "history = model.fit(dataset, epochs=EPOCHS, callbacks=callbacks_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Model: \"sequential_1\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\nembedding_1 (Embedding)      (1, None, 256)            21504     \n_________________________________________________________________\ngru_1 (GRU)                  (1, None, 1024)           3938304   \n_________________________________________________________________\ndense_1 (Dense)              (1, None, 84)             86100     \n=================================================================\nTotal params: 4,045,908\nTrainable params: 4,045,908\nNon-trainable params: 0\n_________________________________________________________________\n"
    }
   ],
   "source": [
    "# rebuild model with latest checkpoints\n",
    "model = build_model(vocab_size, embedding_dim, rnn_units, batch_size=1)\n",
    "\n",
    "model.load_weights(tf.train.latest_checkpoint(checkpoint_dir))\n",
    "\n",
    "model.build(tf.TensorShape([1, None]))\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(model, start_string):\n",
    "  # evaluation step (generating text using the learned model)\n",
    "\n",
    "  # number of characters to generate\n",
    "  num_generate = 1000\n",
    "\n",
    "  # converting our start string to numbers (vectorizing)\n",
    "  input_eval = [char2idx[s] for s in start_string]\n",
    "  input_eval = tf.expand_dims(input_eval, 0)\n",
    "\n",
    "  # empty string to store our results\n",
    "  text_generated = []\n",
    "\n",
    "  # low temperatures results in more predictable text.\n",
    "  # higher temperatures results in more surprising text.\n",
    "  # :D\n",
    "  temperature = 1.0\n",
    "\n",
    "  # here batch size == 1\n",
    "  model.reset_states()\n",
    "  for i in range(num_generate):\n",
    "      predictions = model(input_eval)\n",
    "      # remove the batch dimension\n",
    "      predictions = tf.squeeze(predictions, 0)\n",
    "\n",
    "      # using a categorical distribution to predict the character returned by the model\n",
    "      predictions = predictions / temperature\n",
    "      predicted_id = tf.random.categorical(predictions, num_samples=1)[-1,0].numpy()\n",
    "\n",
    "      # We pass the predicted character as the next input to the model\n",
    "      # along with the previous hidden state\n",
    "      input_eval = tf.expand_dims([predicted_id], 0)\n",
    "\n",
    "      text_generated.append(idx2char[predicted_id])\n",
    "\n",
    "  return (start_string + ''.join(text_generated))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "The University. However, attenden by the coronavirus, or who have family members or loved ones or friends stricken by For of the impace of the Summer.\nThe College of Dentistry Resumes Providing Thoug on a day of pride for d July 6 â€” will be held remotely; administrative personnel with the option of arranging to come to residence halls to retrive the semester in the coming been exsended to main access to their office for recent research to in oncemelty advicorvort the a restrictions in privary to combat the spread of COVID-19.\n\n\nAcademic Ye is virtually Unter pack up thein possessions of New York State mandates. This will continue to communicate about our plonsent a some are moint, and to continue to health effort to assist students who have financial need caused by COVID-19, in the layt through the end of June. Once the trajectory of COVID-19, NYU has made rooms where students packed up the reach of COVID-19; the NYU community will be the belongings back to the increased research activity \n"
    }
   ],
   "source": [
    "print(generate_text(model, start_string='The '))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "WARNING:tensorflow:From /Users/pastel/Library/Python/3.8/lib/python/site-packages/tensorflow/python/ops/resource_variable_ops.py:1813: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\nInstructions for updating:\nIf using Keras pass *_constraint arguments to layers.\nINFO:tensorflow:Assets written to: saved_model/my_model/assets\n"
    }
   ],
   "source": [
    "!mkdir -p saved_model\n",
    "model.save('saved_model/my_model') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.2 64-bit",
   "language": "python",
   "name": "python38264bitdaa33b5c84404be2ba520b5146c0c621"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}